An implementation of transformer in PyTorch

## PyTorchで言語データを扱うには？
torchtextと呼ばれるライブラリを用いるのがよい。
よくある（テキスト, ラベル）で組になったデータを扱うには、

1. テキストデータ、ラベルデータに対し、行う処理を定義
2. `torchtext.data.TabularDataset`を用いtsvファイルを読み込み（多分csvでもいける？）

詳しくはutil.pyを参照。

## 忘れがちなword2vecによるベクトル表現方法の詳細
「ある単語の表現は、その周囲によく出てくる単語の表現と類似している」という考えがもとになっている。

実際にベクトル表現を行う手法としては大きく2つ。

1. Continuous Bag-of-Words (CBoW)
2. Skip-gram

### CBowについて
1. まずは、大量のテキストをそれぞれ単語分割。
  - 例えば「姫坂乃愛 は 世界一 可愛い 女子 小学生 である」のように
2. 埋め込み表現を学習したい単語の周囲の単語から、着目単語を予測するタスクをNNにより学習
3. NNの中間層が埋め込み表現になる

これに加え、原著論文では
- ネガティブサンプリング
- 階層型softmax

と呼ばれる手法を用い学習を効率化している。

### Skip-gramについて
逆に、ある着目単語を入力とし、その周囲の単語を予測するタスクを通し、埋め込み表現を得る手法


### どっちがよい？
EmpiricalにはSkip-gramの方がよいらしい……でもTheoreticalにはよくわかっていない。
やっぱりDeepは難しいね。

## fastTextってなーに？
大きな違いはサブワードを用いるという点。具体的に「姫坂乃愛」を用いサブワードを示してみると、

- 1文字：（姫, 坂, 乃, 愛）
- 2文字：（姫坂, 坂乃, 乃愛）
- 3文字：（姫坂乃, 坂乃愛）
- 4文字：（姫坂乃愛）

のように分割される。（本当は開始記号、終端記号も含める）
これを導入することにより、未知語への対応が可能になる。（すごい）
